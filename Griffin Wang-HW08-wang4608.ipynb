{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99cdbc6",
   "metadata": {},
   "source": [
    "### 2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following *metrics* below: provide your answers and, in your own words, *concisely explain your rationale for your answers.*<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0146e5d",
   "metadata": {},
   "source": [
    "1. **Accuracy** When the general trend is stable and the costs of FN and FP are close.\n",
    "2. **Sensitivity** When FN is more concerned than FP.\n",
    "3. **Specificity** when FP is more concerned than FN.\n",
    "4. **Precision** When it is necessary to minimizing FP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71d78b",
   "metadata": {},
   "source": [
    "### 4. Create an 80/20 split with 80% of the data as a training set *ab_reduced_noNaN_train* and 20% of the data testing set  *ab_reduced_noNaN_test* using either *df.sample(...)* as done in TUT or using *train_test_split(...)* as done in the previous HW, and report on how many observations there are in the training data set and the test data set.<br><br>Tell a ChatBot that you are about to fit a \"scikit-learn\" *DecisionTreeClassifier* model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree *clf* using only the *List Price* variable to predict whether or not a book is a hard cover or paper back book using a *max_depth* of *2*; finally use *tree.plot_tree(clf)* to explain what *predictions* are made based on *List Price* for the fitted *clf* model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecd9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:\n",
      "    List Price\n",
      "5       20.00\n",
      "0       10.99\n",
      "7        8.99\n",
      "2       15.49\n",
      "9        6.99\n",
      "4        7.99\n",
      "3       10.00\n",
      "6        4.99\n",
      "\n",
      "Train y:\n",
      " 5     True\n",
      "0     True\n",
      "7    False\n",
      "2     True\n",
      "9    False\n",
      "4    False\n",
      "3     True\n",
      "6    False\n",
      "Name: H, dtype: bool\n",
      "\n",
      "Test X:\n",
      "    List Price\n",
      "8       12.00\n",
      "1        5.99\n",
      "\n",
      "Test y:\n",
      " 8     True\n",
      "1    False\n",
      "Name: H, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Hard_or_Paper\": [\"H\", \"P\", \"H\", \"H\", \"P\", \"H\", \"P\", \"P\", \"H\", \"P\"],\n",
    "    \"List Price\": [10.99, 5.99, 15.49, 10.0, 7.99, 20.0, 4.99, 8.99, 12.0, 6.99],\n",
    "}\n",
    "ab_reduced_noNaN = pd.DataFrame(data)\n",
    "\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train X:\\n\", X_train)\n",
    "print(\"\\nTrain y:\\n\", y_train)\n",
    "print(\"\\nTest X:\\n\", X_test)\n",
    "print(\"\\nTest y:\\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d631e",
   "metadata": {},
   "source": [
    "### 6. Use previously created *ab_reduced_noNaN_test* to create confusion matrices for *clf* and *clf2*. Report the sensitivity, specificity and accuracy for each of the models<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd3df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Sensitivity  Specificity  Accuracy\n",
      "0   clf          1.0          1.0       1.0\n",
      "1  clf2          1.0          1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "y_pred_clf = clf.predict(X_test) \n",
    "y_pred_clf2 = clf2.predict(X_test) \n",
    "\n",
    "\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "\n",
    "TP_clf = cm_clf[1, 1]\n",
    "TN_clf = cm_clf[0, 0]\n",
    "FP_clf = cm_clf[0, 1]\n",
    "FN_clf = cm_clf[1, 0]\n",
    "\n",
    "TP_clf2 = cm_clf2[1, 1]\n",
    "TN_clf2 = cm_clf2[0, 0]\n",
    "FP_clf2 = cm_clf2[0, 1]\n",
    "FN_clf2 = cm_clf2[1, 0]\n",
    "\n",
    "sensitivity_clf = TP_clf / (TP_clf + FN_clf)\n",
    "specificity_clf = TN_clf / (TN_clf + FP_clf)\n",
    "accuracy_clf = (TP_clf + TN_clf) / cm_clf.sum()\n",
    "\n",
    "sensitivity_clf2 = TP_clf2 / (TP_clf2 + FN_clf2)\n",
    "specificity_clf2 = TN_clf2 / (TN_clf2 + FP_clf2)\n",
    "accuracy_clf2 = (TP_clf2 + TN_clf2) / cm_clf2.sum()\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"clf\", \"clf2\"],\n",
    "    \"Sensitivity\": [round(sensitivity_clf, 3), round(sensitivity_clf2, 3)],\n",
    "    \"Specificity\": [round(specificity_clf, 3), round(specificity_clf2, 3)],\n",
    "    \"Accuracy\": [round(accuracy_clf, 3), round(accuracy_clf2, 3)],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5403e",
   "metadata": {},
   "source": [
    "### GPT https://chatgpt.com/share/67400e76-4c40-8006-be00-22500854681a\n",
    "\n",
    "Hereâ€™s a brief summary of what the code does:\n",
    "\n",
    "Dataset Setup:\n",
    "\n",
    "It assumes a dataset named ab_reduced_noNaN with columns Hard_or_Paper and List Price.\n",
    "The target variable (y) is derived using pd.get_dummies to encode whether the book type is Hard (H = 1, else = 0).\n",
    "The feature (X) is the List Price.\n",
    "Train-Test Split:\n",
    "\n",
    "The data is split into 80% training and 20% testing using train_test_split from sklearn.\n",
    "random_state=42 ensures reproducibility of the split.\n",
    "Outputs:\n",
    "\n",
    "X_train and y_train: Training data (features and target).\n",
    "X_test and y_test: Testing data (features and target).\n",
    "Usage:\n",
    "\n",
    "The train set is used to build a model.\n",
    "The test set evaluates how well the model generalizes to unseen data.\n",
    "If you run the code, youâ€™ll get a clear view of how the data is split. Let me know if youâ€™d like to dive deeper into any step! ðŸ˜Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912f9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
